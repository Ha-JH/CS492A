{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CGAN_one_sided_label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9248bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import new_class\n",
    "import torch\n",
    "from torch.autograd.grad_mode import no_grad\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.activation import LeakyReLU, ReLU\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, backward\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c949bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, n_classes=10, nc=1, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.nc = nc\n",
    "        self.ndf = ndf\n",
    "        \n",
    "        \n",
    "        self.image_net = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32\n",
    "            nn.Conv2d(in_channels=nc, out_channels=ndf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "            # state size. (ndf) x 16 x 16\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
    "\n",
    "        self.label_net = nn.Sequential(\n",
    "            # input is one-hot embedding (n_classes)\n",
    "            nn.Conv2d(in_channels=n_classes, out_channels=ndf, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "            # state size. (ndf) x 16 x 16\n",
    "        )\n",
    "        '''\n",
    "        self.label_condition = nn.Sequential(\n",
    "            nn.Embedding(n_classes, n_classes),\n",
    "            nn.Linear(n_classes, 16 * 16 * ndf)\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(in_channels=ndf * 2, out_channels=ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(in_channels=ndf * 4, out_channels=ndf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(in_channels=ndf * 8, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        img, label = input\n",
    "        label_output = self.label_condition(label)\n",
    "        label_output = label_output.view(-1, self.ndf, 16, 16)\n",
    "        concat = torch.cat((self.image_net(img), label_output), dim=1)\n",
    "        return self.main(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db027f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, n_classes=10, nz=100, ngf=64, nc=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.nz = nz\n",
    "        self.nc = nc\n",
    "        self.ngf = ngf\n",
    "\n",
    "        self.noise_net = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(in_channels=nz, out_channels=ngf * 4, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "            # state size. (ngf*4) x 4 x 4\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
    "\n",
    "        self.label_net = nn.Sequential(\n",
    "            # input is one-hot embedding (emedding_dim)\n",
    "            nn.ConvTranspose2d(in_channels=n_classes, out_channels=ngf * 4, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "            # state size. (ngf*4) x 4 x 4\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        self.label_condition = nn.Sequential(\n",
    "            nn.Embedding(n_classes, n_classes),\n",
    "            nn.Linear(n_classes, 4 * 4 * 4 * ngf)\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(in_channels=ngf * 8, out_channels=ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(in_channels=ngf * 4, out_channels=ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(in_channels=ngf * 2, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        noise, label = input\n",
    "        label_output = self.label_condition(label)\n",
    "        label_output = label_output.view(-1, self.ngf * 4, 4, 4)\n",
    "        concat = torch.cat((self.noise_net(noise), label_output), dim=1)\n",
    "        return self.main(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2641ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### real_label=0.9\n",
    "class CGAN(nn.Module):\n",
    "    def __init__(self, ngpu, device, n_classes=10, embedding_dim=10, lr=0.0002, nc=1, ndf=64, nz=100, ngf=64, beta1=0.5, real_label=1.0):\n",
    "        super(CGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.device = device\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.embeding_dim = embedding_dim\n",
    "        self.lr = lr\n",
    "        self.nc = nc\n",
    "        self.ndf = ndf\n",
    "        self.nz = nz\n",
    "        self.ngf = ngf\n",
    "\n",
    "        # Create the generator\n",
    "        self.netG = Generator(ngpu, n_classes, nz, ngf, nc).to(device)\n",
    "\n",
    "        # Handle multi-gpu if desired\n",
    "        if (device.type == 'cuda') and (ngpu > 1):\n",
    "            self.netG = nn.DataParallel(self.netG, list(range(ngpu)))\n",
    "\n",
    "        # Apply the weights_init function to randomly initialize all weights\n",
    "        #  to mean=0, stdev=0.02.\n",
    "        self.netG.apply(weights_init)\n",
    "\n",
    "        # Create the Discriminator\n",
    "        self.netD = Discriminator(ngpu, n_classes, nc, ndf).to(device)\n",
    "\n",
    "        # Handle multi-gpu if desired\n",
    "        if (device.type == 'cuda') and (ngpu > 1):\n",
    "            self.netD = nn.DataParallel(self.netD, list(range(ngpu)))\n",
    "\n",
    "        # Apply the weights_init function to randomly initialize all weights\n",
    "        #  to mean=0, stdev=0.2.\n",
    "        self.netD.apply(weights_init)\n",
    "\n",
    "\n",
    "        # Initialize BCELoss function\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Create batch of latent vectors that we will use to visualize\n",
    "        #  the progression of the generator\n",
    "        self.fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "        self.fixed_label = torch.arange(64, device=device)\n",
    "        self.fixed_label = torch.remainder(self.fixed_label, n_classes)\n",
    "        self.fixed_label= self.fixed_label.view(-1, 1)\n",
    "\n",
    "        # Establish convention for real and fake labels during training\n",
    "        self.real_label = real_label\n",
    "        self.fake_label = 0.\n",
    "\n",
    "        # Setup Adam optimizers for both G and D\n",
    "        self.optimizerD = optim.Adam(self.netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "        self.optimizerG = optim.Adam(self.netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "    def train(self, dataloader, num_epochs, plot=False):\n",
    "\n",
    "        # Training Loop\n",
    "\n",
    "        # Lists to keep track of progress\n",
    "        img_list = []\n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "        iters = 0\n",
    "\n",
    "        print(\"Starting Training Loop...\")\n",
    "        # For each epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            # For each batch in the dataloader\n",
    "            for i, (real_images, labels) in enumerate(dataloader):\n",
    "\n",
    "                ############################\n",
    "                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                ###########################\n",
    "                ## Train with all-real batch\n",
    "                self.netD.zero_grad()\n",
    "                # Format batch\n",
    "                real_images = real_images.to(self.device)\n",
    "                b_size = real_images.size(0)\n",
    "                labels = labels.to(self.device)\n",
    "                labels = labels.unsqueeze(1).long()\n",
    "\n",
    "                real_target = Variable(torch.ones(b_size, 1).to(self.device))\n",
    "                fake_target = Variable(torch.zeros(b_size, 1).to(self.device))\n",
    "\n",
    "                output = self.netD((real_images, labels)).view(-1, 1)\n",
    "                errD_real = self.criterion(output, real_target)\n",
    "                errD_real.backward()\n",
    "                D_x = output.mean().item()\n",
    "\n",
    "                noise_vector = torch.randn(b_size, self.nz, 1, 1, device=self.device)\n",
    "                noise_vector = noise_vector.to(self.device)\n",
    "\n",
    "                generated_image = self.netG((noise_vector, labels))\n",
    "                output = self.netD((generated_image.detach(), labels)).view(-1, 1)\n",
    "                errD_fake = self.criterion(output, fake_target)\n",
    "                errD_fake.backward()   \n",
    "                D_G_z1 = output.mean().item()\n",
    "\n",
    "                errD = errD_real + errD_fake\n",
    "\n",
    "                self.optimizerD.step()\n",
    "\n",
    "\n",
    "                self.optimizerG.zero_grad()\n",
    "                output = self.netD((generated_image, labels)).view(-1, 1)\n",
    "                errG = self.criterion(output, real_target)\n",
    "                errG.backward()\n",
    "                D_G_z2 = output.mean().item()\n",
    "                self.optimizerG.step()\n",
    "\n",
    "                # Output training stats\n",
    "                if i % 50 == 0:\n",
    "                    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                        % (epoch, num_epochs, i, len(dataloader),\n",
    "                            errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "                # Save Losses for plotting later\n",
    "                G_losses.append(errG.item())\n",
    "                D_losses.append(errD.item())\n",
    "\n",
    "                # Check how the generator is doing by saving G's output on fixed_noise\n",
    "                if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "                    with torch.no_grad():\n",
    "                        fake = self.netG((self.fixed_noise, self.fixed_label)).detach().cpu()\n",
    "                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "                iters += 1\n",
    "\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "            plt.plot(G_losses,label=\"G\")\n",
    "            plt.plot(D_losses,label=\"D\")\n",
    "            plt.xlabel(\"iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        return img_list\n",
    "    \n",
    "    def create_dataloader(self, num_samples=1000, batch_size=128):\n",
    "        labels = torch.arange(num_samples, device=self.device)\n",
    "        labels = torch.remainder(labels, self.n_classes)\n",
    "        noises = torch.randn(num_samples, self.nz, 1, 1, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            fakes = self.netG((noises, labels)).detach().cpu()\n",
    "        \n",
    "        data = {\n",
    "            'images': fakes,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        fake_dataset = FakeDataset(data)\n",
    "\n",
    "        fake_dataloader = torch.utils.data.DataLoader(fake_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "        return fake_dataset, fake_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "         self.transform = transform\n",
    "         self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.data['images'][idx]\n",
    "        label = self.data['labels'][idx]\n",
    "        sample = (image, label)\n",
    "\n",
    "        return sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
